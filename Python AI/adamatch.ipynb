{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"adamatch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMB5piSABo/emVkY/HHQfPm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"easqifpo1uIC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658301255219,"user_tz":-540,"elapsed":95906,"user":{"displayName":"48","userId":"17678505092892983243"}},"outputId":"b1d3cb7f-8c34-45fa-9656-2723d9c18ae5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 2.1 MB 6.7 MB/s \n","\u001b[K     |████████████████████████████████| 48.3 MB 1.3 MB/s \n","\u001b[K     |████████████████████████████████| 4.6 MB 40.5 MB/s \n","\u001b[K     |████████████████████████████████| 99 kB 10.9 MB/s \n","\u001b[K     |████████████████████████████████| 237 kB 66.3 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 71.7 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 24.4 MB/s \n","\u001b[K     |████████████████████████████████| 636 kB 37.7 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 48.8 MB/s \n","\u001b[K     |████████████████████████████████| 511.7 MB 5.7 kB/s \n","\u001b[K     |████████████████████████████████| 92 kB 11.7 MB/s \n","\u001b[K     |████████████████████████████████| 438 kB 59.9 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 49.4 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 41.6 MB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q tf-models-official"]},{"cell_type":"code","source":["pip install keras-cv --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvdV-0R86tm2","executionInfo":{"status":"ok","timestamp":1658308985289,"user_tz":-540,"elapsed":6397,"user":{"displayName":"48","userId":"17678505092892983243"}},"outputId":"d154b205-b356-45dc-bf3a-150023150a49"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras-cv\n","  Downloading keras_cv-0.2.8-py3-none-any.whl (172 kB)\n","\u001b[K     |████████████████████████████████| 172 kB 6.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-cv) (21.3)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from keras-cv) (1.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-cv) (3.0.9)\n","Installing collected packages: keras-cv\n","Successfully installed keras-cv-0.2.8\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","tf.random.set_seed(42)\n","\n","import numpy as np\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","from keras_cv.layers import RandAugment\n","\n","import tensorflow_datasets as tfds\n","\n","tfds.disable_progress_bar()"],"metadata":{"id":"JoX7HJlakx68","executionInfo":{"status":"ok","timestamp":1658309004834,"user_tz":-540,"elapsed":323,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = keras.datasets.mnist.load_data()\n","\n","mnist_x_train = tf.expand_dims(mnist_x_train, -1)\n","mnist_x_test = tf.expand_dims(mnist_x_test, -1)\n","\n","mnist_y_train = tf.one_hot(mnist_y_train, 10).numpy()\n","\n","svhn_train, svhn_test = tfds.load(\n","    \"svhn_cropped\", split=[\"train\", \"test\"], as_supervised=True\n",")"],"metadata":{"id":"Yy87NlQPEhTX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658301752303,"user_tz":-540,"elapsed":493527,"user":{"displayName":"48","userId":"17678505092892983243"}},"outputId":"a4de8070-e656-4377-c681-b238d12faf9d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n","\u001b[1mDownloading and preparing dataset svhn_cropped/3.0.0 (download: 1.47 GiB, generated: Unknown size, total: 1.47 GiB) to /root/tensorflow_datasets/svhn_cropped/3.0.0...\u001b[0m\n","Shuffling and writing examples to /root/tensorflow_datasets/svhn_cropped/3.0.0.incompleteM9N0VH/svhn_cropped-train.tfrecord\n","Shuffling and writing examples to /root/tensorflow_datasets/svhn_cropped/3.0.0.incompleteM9N0VH/svhn_cropped-test.tfrecord\n","Shuffling and writing examples to /root/tensorflow_datasets/svhn_cropped/3.0.0.incompleteM9N0VH/svhn_cropped-extra.tfrecord\n","\u001b[1mDataset svhn_cropped downloaded and prepared to /root/tensorflow_datasets/svhn_cropped/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"]}]},{"cell_type":"code","source":["RESIZE_TO = 32\n","SOURCE_BATCH_SIZE = 64\n","TARGET_BATCH_SIZE = 3\n","EPOCHS = 10\n","STEPS_PER_EPOCH = len(mnist_x_train) / SOURCE_BATCH_SIZE\n","TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n","\n","\n","AUTO = tf.data.AUTOTUNE\n","LEARNING_RATE = 0.03\n","\n","WEIGHT_DECAY = 0.0005\n","INIT = \"he_normal\"\n","DEPTH = 28\n","WIDTH_MULT = 2"],"metadata":{"id":"f08sE1j9kLvH","executionInfo":{"status":"ok","timestamp":1658306634080,"user_tz":-540,"elapsed":323,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["augmenter = RandAugment(value_range=(0, 255),augmentations_per_image= 2, magnitude=0.5)\n","\n","def weak_augment(image, source=True):\n","  if image.dtype != tf.float32:\n","    image = tf.cast(image, tf.float32)\n","\n","  if source:\n","    image =tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n","    image = tf.tile(image, [1, 1, 3])\n","  image = tf.image.random_flip_left_right(image)\n","  image = tf.image.random_crop(image, (RESIZE_TO, RESIZE_TO, 3))\n","  return image\n","\n","def strong_augment(image, source=True):\n","  if image.dtype != tf.float32:\n","    image = tf.cast(image, tf.float32)\n","\n","  if source:\n","    image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n","    image = tf.tile(image, [1, 1, 3])\n","  image = augmenter(image)\n","  return image"],"metadata":{"id":"y3d7prB0c8wL","executionInfo":{"status":"ok","timestamp":1658309244625,"user_tz":-540,"elapsed":327,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def create_individual_ds(ds, aug_func, source=True):\n","  if source:\n","    batch_size = SOURCE_BATCH_SIZE\n","  else:\n","    batch_size = TARGET_BATCH_SIZE\n","  ds = ds.shuffle(batch_size * 10, seed=42)\n","\n","  if source:\n","    ds = ds.map(lambda x, y: (aug_func(x), y), num_parallel_calls=AUTO)\n","  else:\n","    df = ds.map(lambda x, y: (aug_func(x, False), y), num_parallel_calls=AUTO)\n","\n","  ds = ds.batch(batch_size).prefetch(AUTO)\n","  return ds"],"metadata":{"id":"mfRunCCkvg71","executionInfo":{"status":"ok","timestamp":1658309091468,"user_tz":-540,"elapsed":5,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["source_ds = tf.data.Dataset.from_tensor_slices((mnist_x_train, mnist_y_train))\n","source_ds_w = create_individual_ds(source_ds, weak_augment)\n","source_ds_s = create_individual_ds(source_ds, strong_augment)\n","final_source_ds = tf.data.Dataset.zip((source_ds_w, source_ds_s))\n","\n","target_ds_w = create_individual_ds(svhn_train, weak_augment, source=False)\n","target_ds_s = create_individual_ds(svhn_train, strong_augment, source=False)\n","final_target_ds = tf.data.Dataset.zip((target_ds_w, target_ds_s))"],"metadata":{"id":"YPj5zAaK5XbR","executionInfo":{"status":"ok","timestamp":1658309258219,"user_tz":-540,"elapsed":11238,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def compute_loss_source(source_labels, logits_source_w, logits_source_s):\n","  loss_func = keras.losses.CategoricalCrossentropy(from_logits=True)\n","  w_loss = loss_func(source_labels, logits_source_w)\n","  s_loss = loss_func(source_labels, logits_source_s)\n","  return w_loss + s_loss\n","\n","def compute_loss_target(target_pseudo_labels_w, logits_target_s, mask):\n","  loss_func = keras.losses.CategoricalCrossentropy(from_logits=True, reduction=\"none\")\n","  target_pseudo_labels_w = tf.stop_gradient(target_pseudo_labels_w)\n","  target_loss = loss_func(target_pseudo_labels_w, logits_target_s)\n","\n","  mask = tf.cast(mask, target_loss.dtype)\n","  target_loss *= mask\n","  return tf.reduce_mean(target_loss, 0)"],"metadata":{"id":"u44Y0Jz07z7H","executionInfo":{"status":"ok","timestamp":1658311258022,"user_tz":-540,"elapsed":322,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["class AdaMatch(keras.Model):\n","  def __init__(self, model, total_steps, tau=0.9):\n","      super(AdaMatch, self).__init__()\n","      self.model = model\n","      self.tau = tau \n","      self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n","      self.total_steps = total_steps\n","      self.current_step = tf.Variable(0, dtype=\"int64\")\n","\n","  @property\n","  def metrics(self):\n","    return [self.loss_tracker]\n","\n","  def compute_mu(self):\n","    pi = tf.constant(np.pi, dtype=\"float32\")\n","    step = tf.cast(self.current_step, dtype=\"float32\")\n","    return 0.5 - tf.cos(tf.math.minimum(pi, (2 * pi * step) / self.total_steps)) / 2\n","\n","  def train_step(self, data):\n","    source_ds, target_ds = data\n","    (source_w, source_labels), (source_s, _) = source_ds\n","    (\n","        (target_w, _),\n","        (target_s, _),\n","    ) = target_ds\n","\n","    combined_images = tf.concat([source_w, source_s, target_w, target_s], 0)\n","    combined_source = tf.concat([source_w, source_s], 0)\n","\n","    total_source = tf.shape(combined_source)[0]\n","    total_target = tf.shape(tf.concat([target_w, target_s], 0))[0]\n","\n","    with tf.GradientTape() as tape:\n","        combined_logits = self.model(combined_images, training=True)\n","        z_d_prime_source = self.model(\n","            combined_source, training=False\n","        )\n","        z_prime_source = combined_logits[:total_source]\n","\n","        lambd = tf.random.uniform((total_source, 10), 0, 1)\n","        final_source_logits = (lambd * z_prime_source) + (\n","            (1 - lambd) * z_d_prime_source\n","        )\n","\n","        y_hat_source_w = tf.nn.softmax(final_source_logits[: tf.shape(source_w)[0]])\n","\n","        logits_target = combined_logits[total_source:]\n","        logits_target_w = logits_target[: tf.shape(target_w)[0]]\n","        y_hat_target_w = tf.nn.softmax(logits_target_w)\n","\n","        expectation_ratio = tf.reduce_mean(y_hat_source_w) / tf.reduce_mean(\n","            y_hat_target_w\n","        )\n","        y_tilde_target_w = tf.math.l2_normalize(\n","            y_hat_target_w * expectation_ratio, 1\n","        )\n","\n","        row_wise_max = tf.reduce_max(y_hat_source_w, axis=-1)\n","        final_sum = tf.reduce_mean(row_wise_max, 0)\n","        c_tau = self.tau * final_sum\n","        mask = tf.reduce_max(y_tilde_target_w, axis=-1) >= c_tau\n","\n","        source_loss = compute_loss_source(\n","            source_labels,\n","            final_source_logits[: tf.shape(source_w)[0]],\n","            final_source_logits[tf.shape(source_w)[0] :],\n","        )\n","        target_loss = compute_loss_target(\n","            y_tilde_target_w, logits_target[tf.shape(target_w)[0] :], mask\n","        )\n","\n","        t = self.compute_mu()\n","        total_loss = source_loss + (t * target_loss)\n","        self.current_step.assign_add(\n","            1\n","        )  \n","\n","    gradients = tape.gradient(total_loss, self.model.trainable_variables)\n","    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n","\n","    self.loss_tracker.update_state(total_loss)\n","    return {\"loss\": self.loss_tracker.result()}"],"metadata":{"id":"kZlBebslDbSE","executionInfo":{"status":"ok","timestamp":1658320569752,"user_tz":-540,"elapsed":293,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":38,"outputs":[]}]}